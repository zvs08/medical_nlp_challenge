import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.neighbors import KNeighborsClassifier
from utils import get_logits

df = pd.read_csv('data/data.csv')

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained('UFNLP/gatortronS')
for target in ['cancer', 'diabetes']:
    # Load the trained models
    model = AutoModelForSequenceClassification.from_pretrained('./results/' + target + '_model')
    model.eval()
    
    # Define a function to encode text
    def encode_texts(texts, tokenizer, max_len=512):
        encodings = tokenizer.batch_encode_plus(
            texts,
            add_special_tokens=True,
            max_length=max_len,
            padding='max_length',
            return_token_type_ids=False,
            return_attention_mask=True,
            truncation=True,
            return_tensors='pt'
        )
        return encodings
    
    def get_logits(texts, model_checkpoint):
        encodings = encode_texts(texts, tokenizer)
        inputs = {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']}
    
        with torch.no_grad():
            outputs = model_checkpoint.bert(**inputs)
            logits = outputs.pooler_output
        return logits
    
    # Collect logits for the entire dataset in batches
    batch_size = 16  # Adjust the batch size based on your memory capacity
    logits_list = []
    label = []
    is_labeled_list = []
    
    for start_idx in tqdm(range(0, len(df), batch_size)):
        end_idx = min(start_idx + batch_size, len(df))
        texts = df.iloc[start_idx:end_idx]['text'].tolist()
        logits = get_logits(texts, model)
        logits_list.append(logits.numpy())

    logits_array = np.vstack(logits_list)
    
    label = np.array(df['has_' + target])
    is_labeled = pd.notna(df['has_' + target])
    labeled_logits = logits_array[is_labeled]
    unlabeled_logits = logits_array[~is_labeled]
    
    knn = KNeighborsClassifier(n_neighbors=5)
    
    # Use only the labeled data for training
    knn.fit(labeled_logits, label[is_labeled])
    
    # Predict the labels for the unlabeled data
    predictions = knn.predict(unlabeled_logits)
    df['pred_has_' + target] = None
    # Combine predictions with the original dataframe
    df.loc[~is_labeled, 'pred_has_' + target] = predictions
df.to_csv('data/data_res.csv', index=False)
